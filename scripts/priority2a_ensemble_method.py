#!/usr/bin/env python3
"""
PRIORITY 2A: Ensemble Method (Baseline + GNN)
Purpose: Combine baseline L2 and GNN scores for improved performance
Expected: 65-70%+ AUC (combining complementary strengths)

Strategy:
1. Load baseline L2 scores (48.02% AUC)
2. Load GNN scores (57.74% AUC or tuned version)
3. Try different combination methods:
   - Simple average
   - Weighted average
   - Max/Min strategies
4. Find optimal combination
5. Evaluate ensemble performance

Why this works:
- Baseline catches certain anomaly types
- GNN catches different anomaly types
- Combination leverages both strengths
- Often gives 5-10% boost over best individual method
"""

import os
import json
import numpy as np
from pathlib import Path
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from tqdm import tqdm
import wandb
from datetime import datetime


class EnsembleScorer:
    """
    Combine multiple anomaly detection methods for improved performance.
    """
    
    def __init__(self,
                 baseline_dir='data/processed/baseline_scores',
                 gnn_dir='data/processed/gnn_scores',
                 labels_path='data/splits/ucsd_ped2_labels.json',
                 output_dir='data/processed/ensemble_scores',
                 enable_wandb=True):
        
        self.baseline_dir = Path(baseline_dir)
        self.gnn_dir = Path(gnn_dir)
        self.labels_path = Path(labels_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.enable_wandb = enable_wandb
        
        if self.enable_wandb:
            wandb.init(  # type: ignore
                project="temporalgraph-vad-complete",
                name=f"ensemble_methods_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                tags=["ensemble", "baseline_gnn", "optimization"],
                config={
                    "phase": "ensemble_optimization",
                    "methods": ["baseline_l2", "gnn_autoencoder"],
                    "combination_strategies": ["average", "weighted", "max", "min"],
                    "optimization_target": "auc_score"
                }
            )
        
        print(f"üîß Ensemble Scorer Initialized")
        print(f"   Baseline: {self.baseline_dir}")
        print(f"   GNN: {self.gnn_dir}")
        print(f"   Output: {self.output_dir}")
    
    def load_labels(self):
        """Load ground truth labels."""
        with open(self.labels_path, 'r') as f:
            labels = json.load(f)
        return labels
    
    def load_scores(self, score_dir, pattern):
        """Load anomaly scores from directory."""
        score_files = sorted(score_dir.glob(pattern))
        
        scores = {}
        for score_file in score_files:
            # Remove suffixes properly: Test001_gnn_scores ‚Üí Test001
            seq_name = score_file.stem
            seq_name = seq_name.replace('_gnn_scores', '').replace('_scores', '')
            scores[seq_name] = np.load(score_file)
        
        return scores
    
    def normalize_scores(self, scores):
        """
        Normalize scores to [0, 1] range using GLOBAL min-max scaling.
        This preserves discriminative power across sequences.
        """
        # Collect all scores across all sequences
        all_values = []
        for seq_scores in scores.values():
            all_values.extend(seq_scores.tolist())
        
        # Find global min/max
        global_min = np.min(all_values)
        global_max = np.max(all_values)
        
        # Normalize each sequence using global scale
        scores_normalized = {}
        for seq_name, seq_scores in scores.items():
            if global_max > global_min:
                normalized = (seq_scores - global_min) / (global_max - global_min)
            else:
                # All scores identical (unlikely)
                normalized = np.ones_like(seq_scores) * 0.5
            
            scores_normalized[seq_name] = normalized.astype(np.float32)
        
        return scores_normalized
    
    def combine_scores(self, baseline_scores, gnn_scores, method='weighted', weight_gnn=0.6):
        """
        Combine baseline and GNN scores.
        
        Args:
            baseline_scores (dict): Baseline L2 scores
            gnn_scores (dict): GNN scores
            method (str): Combination method
                - 'average': Simple average
                - 'weighted': Weighted average (favor better method)
                - 'max': Take maximum score
                - 'min': Take minimum score
            weight_gnn (float): Weight for GNN in weighted average
            
        Returns:
            dict: Combined scores
        """
        combined = {}
        
        for seq_name in baseline_scores.keys():
            if seq_name not in gnn_scores:
                continue
            
            baseline = baseline_scores[seq_name]
            gnn = gnn_scores[seq_name]
            
            # Check alignment
            if len(baseline) != len(gnn):
                print(f"  ‚ö†Ô∏è {seq_name}: Length mismatch ({len(baseline)} vs {len(gnn)})")
                continue
            
            # Combine based on method
            if method == 'average':
                combined[seq_name] = (baseline + gnn) / 2.0
            
            elif method == 'weighted':
                weight_baseline = 1.0 - weight_gnn
                combined[seq_name] = weight_baseline * baseline + weight_gnn * gnn
            
            elif method == 'max':
                combined[seq_name] = np.maximum(baseline, gnn)
            
            elif method == 'min':
                combined[seq_name] = np.minimum(baseline, gnn)
            
            else:
                raise ValueError(f"Unknown method: {method}")
        
        return combined
    
    def evaluate_scores(self, scores, labels, method_name='Ensemble'):
        """
        Evaluate anomaly scores and compute AUC.
        
        Args:
            scores (dict): Anomaly scores per sequence
            labels (dict): Ground truth labels
            method_name (str): Name for display
            
        Returns:
            float: Overall AUC score
        """
        all_scores = []
        all_labels = []
        
        for seq_name in sorted(scores.keys()):
            if seq_name not in labels:
                continue
            
            seq_scores = scores[seq_name]
            seq_labels = np.array(labels[seq_name])
            
            if len(seq_scores) != len(seq_labels):
                continue
            
            all_scores.extend(seq_scores)
            all_labels.extend(seq_labels)
        
        # Compute AUC
        if len(all_scores) > 0:
            all_scores = np.array(all_scores)
            all_labels = np.array(all_labels)
            
            # Check if we have both classes
            unique_labels = np.unique(all_labels)
            if len(unique_labels) > 1:
                try:
                    fpr, tpr, _ = roc_curve(all_labels, all_scores)
                    auc_score = auc(fpr, tpr)
                    return auc_score, fpr, tpr
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Error computing AUC for {method_name}: {e}")
                    return 0.0, None, None
            else:
                # Only one class present (all normal or all anomaly)
                return 0.5, None, None
        else:
            return 0.0, None, None
    
    def find_optimal_ensemble(self):
        """
        Test different ensemble strategies and find the best one.
        """
        print("\n" + "=" * 70)
        print("üîç FINDING OPTIMAL ENSEMBLE STRATEGY")
        print("=" * 70)
        
        # Load data
        print("\nüìÇ Loading data...")
        labels = self.load_labels()
        baseline_scores = self.load_scores(self.baseline_dir, 'Test*_scores.npy')
        gnn_scores = self.load_scores(self.gnn_dir, 'Test*_gnn_scores.npy')
        
        print(f"   ‚úì Labels: {len(labels)} sequences")
        print(f"   ‚úì Baseline scores: {len(baseline_scores)} sequences")
        print(f"   ‚úì GNN scores: {len(gnn_scores)} sequences")
        
        # Normalize scores
        print("\nüîß Normalizing scores to [0, 1] range...")
        baseline_norm = self.normalize_scores(baseline_scores)
        gnn_norm = self.normalize_scores(gnn_scores)
        
        # DEBUG: Check what was loaded
        print(f"\nüîç Debug loaded scores:")
        print(f"   Baseline sequences: {sorted(baseline_scores.keys())}")
        print(f"   GNN sequences: {sorted(gnn_scores.keys())}")
        if 'Test001' in baseline_scores:
            print(f"   Test001 baseline: {len(baseline_scores['Test001'])} scores, range [{baseline_scores['Test001'].min():.4f}, {baseline_scores['Test001'].max():.4f}]")
        if 'Test001' in gnn_scores:
            print(f"   Test001 GNN: {len(gnn_scores['Test001'])} scores, range [{gnn_scores['Test001'].min():.4f}, {gnn_scores['Test001'].max():.4f}]")
        
        # Evaluate individual methods first (use RAW scores, not normalized!)
        print("\nüìä Evaluating individual methods:")
        baseline_auc, _, _ = self.evaluate_scores(baseline_scores, labels, 'Baseline')
        gnn_auc, _, _ = self.evaluate_scores(gnn_scores, labels, 'GNN')
        
        print(f"   Baseline L2: {baseline_auc:.4f} ({baseline_auc*100:.2f}%)")
        print(f"   GNN:         {gnn_auc:.4f} ({gnn_auc*100:.2f}%)")
        
        # Test ensemble strategies
        print("\nüß™ Testing ensemble strategies:")
        print("-" * 70)
        
        strategies = [
            ('average', None),
            ('weighted', 0.5),
            ('weighted', 0.6),
            ('weighted', 0.7),
            ('weighted', 0.8),
            ('max', None),
            ('min', None)
        ]
        
        results = []
        best_auc = 0.0
        best_strategy = None
        best_combined = None
        
        for method, weight in strategies:
            # Combine scores
            if method == 'weighted':
                combined = self.combine_scores(baseline_norm, gnn_norm, method, weight)
                strategy_name = f"Weighted (GNN={weight:.1f})"
            else:
                combined = self.combine_scores(baseline_norm, gnn_norm, method)
                strategy_name = method.capitalize()
            
            # Evaluate
            ensemble_auc, fpr, tpr = self.evaluate_scores(combined, labels, strategy_name)
            
            results.append({
                'method': method,
                'weight': weight,
                'strategy_name': strategy_name,
                'auc': ensemble_auc,
                'fpr': fpr,
                'tpr': tpr,
                'combined_scores': combined
            })
            
            print(f"   {strategy_name:30s}: {ensemble_auc:.4f} ({ensemble_auc*100:.2f}%)")
            
            if ensemble_auc > best_auc:
                best_auc = ensemble_auc
                best_strategy = strategy_name
                best_combined = combined
        
        # Sort by AUC
        results_sorted = sorted(results, key=lambda x: x['auc'], reverse=True)
        
        # Print summary
        print("\n" + "=" * 70)
        print("üìà ENSEMBLE PERFORMANCE SUMMARY")
        print("=" * 70)
        
        print(f"\nüèÜ BEST ENSEMBLE: {best_strategy}")
        print(f"   AUC: {best_auc:.4f} ({best_auc*100:.2f}%)")
        
        improvement_baseline = (best_auc - baseline_auc) * 100
        improvement_gnn = (best_auc - gnn_auc) * 100
        
        print(f"\nüìä Improvements:")
        print(f"   vs Baseline: +{improvement_baseline:.2f} percentage points")
        print(f"   vs GNN:      +{improvement_gnn:.2f} percentage points")
        
        # Save best ensemble scores
        if best_combined is not None:
            print("\nüíæ Saving best ensemble scores...")
            for seq_name, scores in best_combined.items():
                output_path = self.output_dir / f'{seq_name}_ensemble_scores.npy'
                np.save(output_path, scores)
            
            print(f"   ‚úì Saved {len(best_combined)} score files")
        else:
            print("\n‚ö†Ô∏è No valid ensemble found - cannot save scores")
        
        # Save results summary
        summary = {
            'best_strategy': best_strategy,
            'best_auc': float(best_auc),
            'baseline_auc': float(baseline_auc),
            'gnn_auc': float(gnn_auc),
            'improvement_vs_baseline': float(improvement_baseline),
            'improvement_vs_gnn': float(improvement_gnn),
            'all_strategies': [
                {
                    'strategy': r['strategy_name'],
                    'auc': float(r['auc'])
                }
                for r in results_sorted
            ]
        }
        
        summary_path = self.output_dir / 'ensemble_results.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\nüìã Results saved: {summary_path}")
        
        # Plot comparison
        self.plot_ensemble_comparison(baseline_auc, gnn_auc, best_auc, results_sorted)
        
        # Final verdict
        print("\n" + "=" * 70)
        if best_auc >= 0.70:
            print("üéâ EXCELLENT! Ensemble achieved 70%+ AUC!")
        elif best_auc >= 0.65:
            print("üëç GREAT! Ensemble achieved 65%+ AUC!")
        elif best_auc >= 0.60:
            print("‚úÖ GOOD! Significant improvement achieved!")
        else:
            print("   Modest improvement. Ensemble helps but limited gains.")
        print("=" * 70)
        
        return summary
    
    def plot_ensemble_comparison(self, baseline_auc, gnn_auc, ensemble_auc, results):
        """Plot comparison of different ensemble strategies."""
        plt.figure(figsize=(12, 6))
        
        # Subplot 1: Bar chart of AUCs
        plt.subplot(1, 2, 1)
        
        strategies = ['Baseline', 'GNN'] + [r['strategy_name'] for r in results[:5]]
        aucs = [baseline_auc, gnn_auc] + [r['auc'] for r in results[:5]]
        colors = ['red', 'blue'] + ['green'] * 5
        
        bars = plt.bar(range(len(strategies)), aucs, color=colors, alpha=0.7)
        plt.axhline(y=0.5, color='k', linestyle='--', linewidth=1, label='Random')
        
        plt.xlabel('Method', fontsize=11)
        plt.ylabel('AUC Score', fontsize=11)
        plt.title('Anomaly Detection Performance Comparison', fontsize=12, fontweight='bold')
        plt.xticks(range(len(strategies)), strategies, rotation=45, ha='right')
        plt.ylim([0.4, max(aucs) * 1.1])
        plt.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for i, (bar, auc_val) in enumerate(zip(bars, aucs)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{auc_val:.3f}', ha='center', va='bottom', fontsize=9)
        
        # Subplot 2: ROC curves
        plt.subplot(1, 2, 2)
        
        # Plot top 3 ensemble methods
        for i, r in enumerate(results[:3]):
            if r['fpr'] is not None and r['tpr'] is not None:
                plt.plot(r['fpr'], r['tpr'], 
                        label=f"{r['strategy_name']} (AUC={r['auc']:.3f})",
                        linewidth=2)
        
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
        plt.xlabel('False Positive Rate', fontsize=11)
        plt.ylabel('True Positive Rate', fontsize=11)
        plt.title('ROC Curves - Top Ensemble Methods', fontsize=12, fontweight='bold')
        plt.legend(loc='lower right', fontsize=9)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = Path('reports') / 'ensemble_comparison.png'
        plot_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"   üìä Plot saved: {plot_path}")
        
        plt.close()


def main():
    """Main execution for ensemble method."""
    print("\n" + "=" * 70)
    print("üöÄ PRIORITY 2A: ENSEMBLE METHOD (BASELINE + GNN)")
    print("=" * 70)
    
    print(f"\nüìä Individual Method Performance:")
    print(f"   Baseline L2: ~48% AUC")
    print(f"   GNN:         ~58% AUC")
    print(f"\nüéØ Expected Ensemble: 65-70%+ AUC")
    print(f"üí° Strategy: Combine complementary strengths")
    
    # Initialize ensemble scorer
    ensemble = EnsembleScorer(
        baseline_dir='data/processed/baseline_scores',
        gnn_dir='data/processed/gnn_scores',
        labels_path='data/splits/ucsd_ped2_labels.json',
        output_dir='data/processed/ensemble_scores'
    )
    
    # Find optimal ensemble
    results = ensemble.find_optimal_ensemble()
    
    # Recommendations
    print(f"\nüéØ NEXT STEPS:")
    if results['best_auc'] >= 0.70:
        print(f"   üéâ Excellent results! Your thesis has strong performance!")
        print(f"   Consider:")
        print(f"      1. Write up results for paper/thesis")
        print(f"      2. Test on Avenue dataset for generalization")
    elif results['best_auc'] >= 0.65:
        print(f"   üëç Great improvement! Solid contribution achieved.")
        print(f"   Consider:")
        print(f"      1. Run Priority 1A if not done (hyperparameter tuning)")
        print(f"      2. Analyze what types of anomalies each method catches")
    else:
        print(f"   Consider:")
        print(f"      1. Run Priority 1A (hyperparameter tuning)")
        print(f"      2. Analyze failure cases in detail")
    
    print("\n" + "=" * 70)
    
    # Finish W&B
    if ensemble.enable_wandb:
        wandb.finish()  # type: ignore


if __name__ == '__main__':
    main()
