#!/usr/bin/env python3\n\"\"\"\nplot_vad_scores.py - Visualization Module for Anomaly Scores\nPlots baseline anomaly scores vs. ground truth labels for inspection and analysis.\n\nThis module provides comprehensive visualization capabilities for understanding\nanomalous score patterns and model behavior.\n\"\"\"\n\nimport os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nimport wandb\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\n# Set style for better-looking plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nclass VADScorePlotter:\n    \"\"\"Visualization module for video anomaly detection scores\"\"\"\n    \n    def __init__(self, scores_dir: str = 'data/processed/baseline_scores',\n                 output_dir: str = 'data/processed/plots',\n                 use_wandb: bool = False):\n        self.scores_dir = Path(scores_dir)\n        self.labels_file = Path('data/splits/ucsd_ped2_labels.json')\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.use_wandb = use_wandb\n        \n        # Load data\n        self.labels_data = self._load_labels()\n        self.scores_data = self._load_scores()\n    \n    def _load_labels(self) -> Dict[str, List[int]]:\n        \"\"\"Load ground truth labels\"\"\"\n        with open(self.labels_file) as f:\n            return json.load(f)\n    \n    def _load_scores(self) -> Dict[str, np.ndarray]:\n        \"\"\"Load anomaly scores\"\"\"\n        scores = {}\n        for score_file in self.scores_dir.glob('*_scores.npy'):\n            seq_name = score_file.stem.replace('_scores', '')\n            scores[seq_name] = np.load(score_file)\n        return scores\n    \n    def plot_sequence_scores(self, seq_name: str, save_plot: bool = True) -> plt.Figure:\n        \"\"\"Plot anomaly scores vs ground truth for a single sequence\"\"\"\n        if seq_name not in self.scores_data or seq_name not in self.labels_data:\n            raise ValueError(f\"Data not found for sequence {seq_name}\")\n        \n        scores = self.scores_data[seq_name]\n        labels = np.array(self.labels_data[seq_name])\n        frames = np.arange(len(scores))\n        \n        fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n        \n        # Plot 1: Anomaly scores timeline\n        axes[0].plot(frames, scores, 'b-', linewidth=1.5, alpha=0.8, label='Anomaly Score')\n        axes[0].fill_between(frames, 0, scores, alpha=0.3)\n        axes[0].set_ylabel('Anomaly Score')\n        axes[0].set_title(f'{seq_name} - Anomaly Scores Timeline')\n        axes[0].grid(True, alpha=0.3)\n        axes[0].legend()\n        \n        # Plot 2: Ground truth labels\n        axes[1].plot(frames, labels, 'r-', linewidth=2, label='Ground Truth')\n        axes[1].fill_between(frames, 0, labels, alpha=0.5, color='red')\n        axes[1].set_ylabel('Anomaly Label')\n        axes[1].set_title(f'{seq_name} - Ground Truth Labels')\n        axes[1].set_ylim(-0.1, 1.1)\n        axes[1].grid(True, alpha=0.3)\n        axes[1].legend()\n        \n        # Plot 3: Combined view with highlighted anomalies\n        ax3_twin = axes[2].twinx()\n        \n        # Plot scores\n        line1 = axes[2].plot(frames, scores, 'b-', linewidth=1.5, alpha=0.8, label='Anomaly Score')\n        axes[2].set_ylabel('Anomaly Score', color='blue')\n        axes[2].tick_params(axis='y', labelcolor='blue')\n        \n        # Highlight anomalous regions\n        anomaly_frames = frames[labels == 1]\n        if len(anomaly_frames) > 0:\n            axes[2].scatter(anomaly_frames, scores[labels == 1], \n                          color='red', s=20, alpha=0.7, zorder=5, label='Anomalous Frames')\n        \n        # Plot ground truth on secondary axis\n        line2 = ax3_twin.plot(frames, labels, 'r-', linewidth=2, alpha=0.7, label='Ground Truth')\n        ax3_twin.set_ylabel('Ground Truth', color='red')\n        ax3_twin.set_ylim(-0.1, 1.1)\n        ax3_twin.tick_params(axis='y', labelcolor='red')\n        \n        axes[2].set_xlabel('Frame Number')\n        axes[2].set_title(f'{seq_name} - Anomaly Scores vs Ground Truth')\n        axes[2].grid(True, alpha=0.3)\n        \n        # Combined legend\n        lines1, labels1 = axes[2].get_legend_handles_labels()\n        lines2, labels2 = ax3_twin.get_legend_handles_labels()\n        axes[2].legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n        \n        plt.tight_layout()\n        \n        if save_plot:\n            plot_file = self.output_dir / f'{seq_name}_scores_timeline.png'\n            plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n            print(f\"  ‚úì Saved timeline plot: {plot_file}\")\n            \n            # Log to W&B if enabled\n            if self.use_wandb:\n                wandb.log({f\"timeline_{seq_name}\": wandb.Image(str(plot_file))})\n        \n        return fig\n    \n    def plot_score_distributions(self, save_plot: bool = True) -> plt.Figure:\n        \"\"\"Plot score distributions for normal vs anomalous frames across all sequences\"\"\"\n        all_normal_scores = []\n        all_anomaly_scores = []\n        \n        for seq_name in self.scores_data.keys():\n            if seq_name not in self.labels_data:\n                continue\n                \n            scores = self.scores_data[seq_name]\n            labels = np.array(self.labels_data[seq_name])\n            \n            normal_mask = labels == 0\n            anomaly_mask = labels == 1\n            \n            if np.any(normal_mask):\n                all_normal_scores.extend(scores[normal_mask])\n            if np.any(anomaly_mask):\n                all_anomaly_scores.extend(scores[anomaly_mask])\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Histogram comparison\n        axes[0, 0].hist(all_normal_scores, bins=50, alpha=0.7, label='Normal', density=True, color='blue')\n        axes[0, 0].hist(all_anomaly_scores, bins=50, alpha=0.7, label='Anomalous', density=True, color='red')\n        axes[0, 0].set_xlabel('Anomaly Score')\n        axes[0, 0].set_ylabel('Density')\n        axes[0, 0].set_title('Score Distribution: Normal vs Anomalous')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # Box plot comparison\n        box_data = [all_normal_scores, all_anomaly_scores]\n        box_labels = ['Normal', 'Anomalous']\n        bp = axes[0, 1].boxplot(box_data, labels=box_labels, patch_artist=True)\n        bp['boxes'][0].set_facecolor('blue')\n        bp['boxes'][1].set_facecolor('red')\n        axes[0, 1].set_ylabel('Anomaly Score')\n        axes[0, 1].set_title('Score Distribution Box Plot')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # Per-sequence mean scores\n        seq_names = []\n        normal_means = []\n        anomaly_means = []\n        \n        for seq_name in sorted(self.scores_data.keys()):\n            if seq_name not in self.labels_data:\n                continue\n                \n            scores = self.scores_data[seq_name]\n            labels = np.array(self.labels_data[seq_name])\n            \n            normal_mask = labels == 0\n            anomaly_mask = labels == 1\n            \n            seq_names.append(seq_name)\n            normal_means.append(scores[normal_mask].mean() if np.any(normal_mask) else 0)\n            anomaly_means.append(scores[anomaly_mask].mean() if np.any(anomaly_mask) else 0)\n        \n        x_pos = np.arange(len(seq_names))\n        width = 0.35\n        \n        axes[1, 0].bar(x_pos - width/2, normal_means, width, label='Normal', alpha=0.7, color='blue')\n        axes[1, 0].bar(x_pos + width/2, anomaly_means, width, label='Anomalous', alpha=0.7, color='red')\n        axes[1, 0].set_xlabel('Sequence')\n        axes[1, 0].set_ylabel('Mean Anomaly Score')\n        axes[1, 0].set_title('Mean Scores by Sequence')\n        axes[1, 0].set_xticks(x_pos)\n        axes[1, 0].set_xticklabels(seq_names, rotation=45)\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # Cumulative distribution\n        normal_sorted = np.sort(all_normal_scores)\n        anomaly_sorted = np.sort(all_anomaly_scores)\n        \n        normal_cdf = np.arange(1, len(normal_sorted) + 1) / len(normal_sorted)\n        anomaly_cdf = np.arange(1, len(anomaly_sorted) + 1) / len(anomaly_sorted)\n        \n        axes[1, 1].plot(normal_sorted, normal_cdf, label='Normal', linewidth=2, color='blue')\n        axes[1, 1].plot(anomaly_sorted, anomaly_cdf, label='Anomalous', linewidth=2, color='red')\n        axes[1, 1].set_xlabel('Anomaly Score')\n        axes[1, 1].set_ylabel('Cumulative Probability')\n        axes[1, 1].set_title('Cumulative Distribution Function')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_plot:\n            plot_file = self.output_dir / 'score_distributions.png'\n            plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n            print(f\"  ‚úì Saved distribution plot: {plot_file}\")\n            \n            if self.use_wandb:\n                wandb.log({\"score_distributions\": wandb.Image(str(plot_file))})\n        \n        return fig\n    \n    def plot_roc_pr_curves(self, save_plot: bool = True) -> plt.Figure:\n        \"\"\"Plot ROC and Precision-Recall curves for each sequence and overall\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # Collect data for overall curves\n        all_labels = []\n        all_scores = []\n        \n        colors = plt.cm.tab10(np.linspace(0, 1, len(self.scores_data)))\n        \n        # ROC curves by sequence\n        for i, (seq_name, color) in enumerate(zip(sorted(self.scores_data.keys()), colors)):\n            if seq_name not in self.labels_data:\n                continue\n                \n            scores = self.scores_data[seq_name]\n            labels = np.array(self.labels_data[seq_name])\n            \n            if len(np.unique(labels)) < 2:\n                continue  # Skip sequences with only one class\n            \n            try:\n                fpr, tpr, _ = roc_curve(labels, scores)\n                axes[0, 0].plot(fpr, tpr, color=color, alpha=0.8, linewidth=2, label=seq_name)\n                \n                precision, recall, _ = precision_recall_curve(labels, scores)\n                axes[0, 1].plot(recall, precision, color=color, alpha=0.8, linewidth=2, label=seq_name)\n                \n                all_labels.extend(labels)\n                all_scores.extend(scores)\n                \n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è Could not plot curves for {seq_name}: {e}\")\n                continue\n        \n        # Format ROC plot\n        axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1)\n        axes[0, 0].set_xlabel('False Positive Rate')\n        axes[0, 0].set_ylabel('True Positive Rate')\n        axes[0, 0].set_title('ROC Curves by Sequence')\n        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # Format PR plot\n        axes[0, 1].set_xlabel('Recall')\n        axes[0, 1].set_ylabel('Precision')\n        axes[0, 1].set_title('Precision-Recall Curves by Sequence')\n        axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # Overall curves\n        if len(all_labels) > 0 and len(np.unique(all_labels)) > 1:\n            all_labels = np.array(all_labels)\n            all_scores = np.array(all_scores)\n            \n            # Overall ROC\n            fpr, tpr, _ = roc_curve(all_labels, all_scores)\n            from sklearn.metrics import auc\n            roc_auc = auc(fpr, tpr)\n            \n            axes[1, 0].plot(fpr, tpr, 'b-', linewidth=3, label=f'Overall AUC = {roc_auc:.3f}')\n            axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n            axes[1, 0].set_xlabel('False Positive Rate')\n            axes[1, 0].set_ylabel('True Positive Rate')\n            axes[1, 0].set_title('Overall ROC Curve')\n            axes[1, 0].legend()\n            axes[1, 0].grid(True, alpha=0.3)\n            \n            # Overall PR\n            precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n            pr_auc = auc(recall, precision)\n            \n            axes[1, 1].plot(recall, precision, 'b-', linewidth=3, label=f'PR AUC = {pr_auc:.3f}')\n            baseline_precision = np.sum(all_labels) / len(all_labels)\n            axes[1, 1].axhline(y=baseline_precision, color='k', linestyle='--', alpha=0.5, \n                             label=f'Baseline = {baseline_precision:.3f}')\n            axes[1, 1].set_xlabel('Recall')\n            axes[1, 1].set_ylabel('Precision')\n            axes[1, 1].set_title('Overall Precision-Recall Curve')\n            axes[1, 1].legend()\n            axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_plot:\n            plot_file = self.output_dir / 'roc_pr_curves.png'\n            plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n            print(f\"  ‚úì Saved ROC/PR curves: {plot_file}\")\n            \n            if self.use_wandb:\n                wandb.log({\"roc_pr_curves\": wandb.Image(str(plot_file))})\n        \n        return fig\n    \n    def generate_comprehensive_report(self, sequences_to_plot: List[str] = None) -> None:\n        \"\"\"Generate a comprehensive visualization report\"\"\"\n        print(\"üìä Generating comprehensive visualization report...\")\n        \n        if sequences_to_plot is None:\n            # Select a few representative sequences\n            sequences_to_plot = ['Test001', 'Test002', 'Test012']  # Mix of different performance levels\n        \n        # Initialize W&B if requested\n        if self.use_wandb:\n            try:\n                wandb.init(project=\"vad-baseline-ucsdped2\", name=\"visualization-report\")\n            except:\n                self.use_wandb = False\n        \n        print(\"\\nüìà Creating visualization plots...\")\n        \n        # 1. Score distributions\n        print(\"  üéØ Score distributions...\")\n        self.plot_score_distributions()\n        \n        # 2. ROC/PR curves\n        print(\"  üìâ ROC and PR curves...\")\n        self.plot_roc_pr_curves()\n        \n        # 3. Individual sequence timelines\n        print(\"  ‚è±Ô∏è Sequence timelines...\")\n        for seq_name in sequences_to_plot:\n            if seq_name in self.scores_data and seq_name in self.labels_data:\n                print(f\"    üìä {seq_name}...\")\n                self.plot_sequence_scores(seq_name)\n                plt.close()  # Close figure to save memory\n        \n        # 4. Summary statistics plot\n        self._plot_summary_statistics()\n        \n        print(f\"\\n‚úÖ Visualization report complete!\")\n        print(f\"üìÅ Plots saved to: {self.output_dir}\")\n        \n        # Finish W&B\n        if self.use_wandb:\n            wandb.finish()  # type: ignore\n    \n    def _plot_summary_statistics(self) -> None:\n        \"\"\"Plot summary statistics across all sequences\"\"\"\n        from sklearn.metrics import roc_auc_score\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        seq_names = []\n        aucs = []\n        frame_counts = []\n        anomaly_rates = []\n        \n        for seq_name in sorted(self.scores_data.keys()):\n            if seq_name not in self.labels_data:\n                continue\n                \n            scores = self.scores_data[seq_name]\n            labels = np.array(self.labels_data[seq_name])\n            \n            seq_names.append(seq_name)\n            frame_counts.append(len(labels))\n            anomaly_rates.append(np.mean(labels))\n            \n            if len(np.unique(labels)) > 1:\n                try:\n                    auc = roc_auc_score(labels, scores)\n                    aucs.append(auc)\n                except:\n                    aucs.append(np.nan)\n            else:\n                aucs.append(np.nan)\n        \n        # AUC by sequence\n        valid_aucs = [auc for auc in aucs if not np.isnan(auc)]\n        valid_seq_names = [name for name, auc in zip(seq_names, aucs) if not np.isnan(auc)]\n        \n        axes[0, 0].bar(valid_seq_names, valid_aucs, alpha=0.7)\n        axes[0, 0].set_ylabel('AUC')\n        axes[0, 0].set_title('AUC by Sequence')\n        axes[0, 0].tick_params(axis='x', rotation=45)\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # Frame counts\n        axes[0, 1].bar(seq_names, frame_counts, alpha=0.7, color='green')\n        axes[0, 1].set_ylabel('Number of Frames')\n        axes[0, 1].set_title('Frame Count by Sequence')\n        axes[0, 1].tick_params(axis='x', rotation=45)\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # Anomaly rates\n        axes[1, 0].bar(seq_names, anomaly_rates, alpha=0.7, color='red')\n        axes[1, 0].set_ylabel('Anomaly Rate')\n        axes[1, 0].set_title('Anomaly Rate by Sequence')\n        axes[1, 0].tick_params(axis='x', rotation=45)\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # AUC vs Anomaly Rate scatter\n        valid_anomaly_rates = [rate for rate, auc in zip(anomaly_rates, aucs) if not np.isnan(auc)]\n        axes[1, 1].scatter(valid_anomaly_rates, valid_aucs, alpha=0.7, s=60)\n        for i, seq in enumerate(valid_seq_names):\n            axes[1, 1].annotate(seq, (valid_anomaly_rates[i], valid_aucs[i]), \n                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n        axes[1, 1].set_xlabel('Anomaly Rate')\n        axes[1, 1].set_ylabel('AUC')\n        axes[1, 1].set_title('AUC vs Anomaly Rate')\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        plot_file = self.output_dir / 'summary_statistics.png'\n        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n        print(f\"  ‚úì Saved summary statistics: {plot_file}\")\n        \n        if self.use_wandb:\n            wandb.log({\"summary_statistics\": wandb.Image(str(plot_file))})\n        \n        plt.close()\n\ndef main():\n    \"\"\"Main execution function for visualization\"\"\"\n    # Check prerequisites\n    if not Path('data/processed/baseline_scores').exists():\n        print(\"‚ùå Error: Baseline scores not found. Run score_ucsd_baseline.py first.\")\n        return\n    \n    if not Path('data/splits/ucsd_ped2_labels.json').exists():\n        print(\"‚ùå Error: Labels not found. Run data preparation scripts first.\")\n        return\n    \n    print(\"üé® Starting visualization pipeline...\")\n    \n    # Create plotter and generate report\n    plotter = VADScorePlotter(use_wandb=False)  # Set to True to enable W&B logging\n    plotter.generate_comprehensive_report()\n    \n    print(\"\\nüéâ Visualization complete!\")\n\nif __name__ == \"__main__\":\n    main()